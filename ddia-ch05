Distributed Data

Shared-nothing architecture == horizontal scale == scale out

Replica VS Partitioning

Purpose of replica
1) geographcally distributed(close to user and reduce latency)
2) one service down and no impact to entire system
3) allowing scale out read and increase throughput


three popular replication algorithm:
1) single leader
2) multi-leader
3) leader less

things need to consider:
1) sync vs async
2) how to deal with failure in repica

leader-base replication OR master/slave replication
- one of repilica is leader and insert data send to leader then send data change to all replica
- replicas are read replicas
- clients can read data from leader or replicas

Replication asynchronously will cause eventually consitent issue
Replication synchronously will cause entire system stop
semi-synchronous: one of follow is replication synchronously and rest of the replications are 
done asynchronously. if current synchronous replication become unavailable then choose one from asynchronous replications becone the new synchronous replications

Set up new replication
1) get db snapshot from master db.
2) copy data from db snapshot to new replicaiton
3) connect new replication to the master db and pull from log sequence number from the Postgres

Replication out of sync
from replica log can figure out the last transaction, so replica can connect to master and request
all data changes after last transaction.

Master unavailable
- need to choose a new leader from replicas and failover need to be done.
1) confirm master is unavailable , most system use timeout, if no response time out for example over 30 sec
we think master is unavailable.
2) choose a new leader, new leader node is the node has newst data replica. let all the nodes agree the new leader
. Here we need consense algorithm like raft
3) reconfigure system allow client to send write request to new leader node. if old leader come back, might still
think it's the master. system need to make sure old leader ackowledge the new leader and old leader become the new slave

Issues might occus:
if use async replication, i


