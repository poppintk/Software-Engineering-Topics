Rate limiting can be implemented using different algorithms, each with its pros and cons. 
The interactions between an interviewer and a candidate help to clarify the type of rate limiters we are trying to build.

Clarification

What kind of rate limiter are we going to design? client-side rate limiter or server-side API rate limiter?
Does the rate limiter throttle API requests based on IP, the user ID, or other properties?
Will the system work in a distributed environment?
Is the rate limiter a separate service or should it be implemented in application code?
Do we need to inform users who are throttled? 
Where to put the rate limiter? on API gateway(rate limiter middleware) or on the API servers?

Requirements
  - Rate limiter middleware
  - low latency. The rate limiter should not slow down HTTP reponse time
  - Distributed rate limiting, The rate limiter can be shared across multiple servers
  - Show clear response msg to users when their requests are throttled
  - High fault tolerance, if there are any problems with the rate limiter, it does not affect the entire system
  - Need appropriate response for limited API request that is rejected and also give response to request that is accept
  - Need rate limiter to be configurable

High-level design 
 Client ----->    Rate limiter middleware ----->  API servers
 
Which algorithm to use?
  - Token bucket
      - Amazon and Stripe use this algorith
      - How it work: A refillable container with limited tokens, in which each request comes in will consume one token out of the container. if there are not enough tokens, the request is dropped.
        after given limit rate(e.g. 10/s). The bucket will gets refilled  
      - Parameters: Bucket size and Refill rate 
      - How many bucket we need?
        - one bucket per one IP address
        - one bucket per system request/second
        - one bucket per feature

      - PROS & CONS
        - Pros
          - easy to implement
          - memory efficient
          - allows burst of traffic for short period of time
  - Leaking bucket
      - Shopify use this algorith
      - How it work:
        - When a request arrives, the system checks if the queue is full. If it is not full, the request is added to the queue. Otherwise, the request is dropped.
         And requests are pulled from the queue and processed at fixed rate(10/s).

      - Parameters: Queue size and fixed rate 
      - PROS & CONS
        - Pros
          - memory efficient given the limited queue size
          - suitable for use cases that a stable fixed rate is needed
        - Cons
          - not good for burst of traffic
        

  - Fixed window counter
      - How it work:
        - divides the timeline into fix-sized time windows and assign a counter for each window. Each request increments the counter by one.Once the counter reaches the pre-defined threshold, new requests are dropped until a new time window starts.
      - PROS & CONS
        - Cons
          - Spike in traffic at the edges of a window could cause maximum double the requests than the allowed quata to go through
  - Sliding window log
      - How it work:
        - keeps track of request timestamps. Timestamp data is usually kept in cache, such as sorted sets of Redis. When a new request comes in, remove all the outdated timestamps. Outdated timestamps are defined as those older than the start of the current time window.
        Add timestamp of the new request to the log. If the log size is the same or lower than the allowed count, a request is accepted. Otherwise, it is rejected. Note: even rejected request also add append a log at the bucket/container
        
      - PROS & CONS
        - Cons
          - consume lots of memory becasue even if a request is rejected, its timestamp might still be stored in memeory
        
        
  The basic idea of rate limiting algorithms is simple. 
  At the high-level, we need a counter to keep track of how many requests are sent from the same user, IP address, etc
  Where shall we store counters?
  Using the database is not a good idea due to slowness of disk access. 
  In-memory cache is chosen because it is fast and supports time-based expiration strategy. 
  For instance, Redis is a popular option to implement rate limiting. 
  It is an in- memory store that offers two commands: INCR and EXPIRE.
   
   Client ----->    Rate limiter middleware ----->  API servers
                          \
                           \-------------> Redis
                           
  • The client sends a request to rate limiting middleware.
  • Rate limiting middleware fetches the counter from the corresponding bucket in Redis and checks if the limit is reached or not.
    • If the limit is reached, the request is rejected.
    • If the limit is not reached, the request is sent to API servers. Meanwhile, the system increments the counter and saves it back to Redis.

Design deep dive

