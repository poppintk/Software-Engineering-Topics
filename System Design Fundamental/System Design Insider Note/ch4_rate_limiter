This article focus on the Application level(HTTP layer) rate limiter

Rate limiting can be implemented using different algorithms, each with its pros and cons. 
The interactions between an interviewer and a candidate help to clarify the type of rate limiters we are trying to build.

Clarification

  What kind of rate limiter are we going to design? client-side rate limiter or server-side API rate limiter?
  Does the rate limiter throttle API requests based on IP, the user ID, or other properties?
  Will the system work in a distributed environment?
  Is the rate limiter a separate service or should it be implemented in application code?
  Do we need to inform users who are throttled? 
  Where to put the rate limiter? on API gateway(rate limiter middleware) or on the API servers?

Requirements
  - Rate limiter middleware
  - low latency. The rate limiter should not slow down HTTP reponse time
  - Distributed rate limiting, The rate limiter can be shared across multiple servers
  - Show clear response msg to users when their requests are throttled
  - High fault tolerance, if there are any problems with the rate limiter, it does not affect the entire system
  - Need appropriate response for limited API request that is rejected and also give response to request that is accept
  - Need rate limiter to be configurable

High-level design 
 Client ----->    Rate limiter middleware ----->  API servers
 
Which algorithm to use?
  - Token bucket
      - Amazon and Stripe use this algorith
      - How it work: A refillable container with limited tokens, in which each request comes in will consume one token out of the container. if there are not enough tokens, the request is dropped.
        after given limit rate(e.g. 10/s). The bucket will gets refilled  
      - Parameters: Bucket size and Refill rate 
      - How many bucket we need?
        - one bucket per one IP address
        - one bucket per system request/second
        - one bucket per feature

      - PROS & CONS
        - Pros
          - easy to implement
          - memory efficient
          - allows burst of traffic for short period of time
  - Leaking bucket
      - Shopify use this algorith
      - How it work:
        - When a request arrives, the system checks if the queue is full. If it is not full, the request is added to the queue. Otherwise, the request is dropped.
         And requests are pulled from the queue and processed at fixed rate(10/s).

      - Parameters: Queue size and fixed rate 
      - PROS & CONS
        - Pros
          - memory efficient given the limited queue size
          - suitable for use cases that a stable fixed rate is needed
        - Cons
          - not good for burst of traffic
        

  - Fixed window counter
      - How it work:
        - divides the timeline into fix-sized time windows and assign a counter for each window. Each request increments the counter by one.Once the counter reaches the pre-defined threshold, new requests are dropped until a new time window starts.
      - PROS & CONS
        - Cons
          - Spike in traffic at the edges of a window could cause maximum double the requests than the allowed quata to go through
  - Sliding window log
      - How it work:
        - keeps track of request timestamps. Timestamp data is usually kept in cache, such as sorted sets of Redis. When a new request comes in, remove all the outdated timestamps. Outdated timestamps are defined as those older than the start of the current time window.
        Add timestamp of the new request to the log. If the log size is the same or lower than the allowed count, a request is accepted. Otherwise, it is rejected. Note: even rejected request also add append a log at the bucket/container
        
      - PROS & CONS
        - Cons
          - consume lots of memory becasue even if a request is rejected, its timestamp might still be stored in memeory
        
        
  The basic idea of rate limiting algorithms is simple. 
  At the high-level, we need a counter to keep track of how many requests are sent from the same user, IP address, etc
  Where shall we store counters?
  Using the database is not a good idea due to slowness of disk access. 
  In-memory cache is chosen because it is fast and supports time-based expiration strategy. 
  For instance, Redis is a popular option to implement rate limiting. 
  It is an in- memory store that offers two commands: INCR and EXPIRE.
   
   Client ----->    Rate limiter middleware ----->  API servers
                          \
                           \-------------> Redis
                           
  • The client sends a request to rate limiting middleware.
  • Rate limiting middleware fetches the counter from the corresponding bucket in Redis and checks if the limit is reached or not.
    • If the limit is reached, the request is rejected. // where is limit saved? on configuration files or DB
    • If the limit is not reached, the request is sent to API servers. Meanwhile, the system increments the counter and saves it back to Redis.

Design deep dive
   How are rate limiting rules created? Where are the rules stored?
      - Example of clients are not allowed to login more than 5 times in 1 minute:
        
        domain: auth // which request has its domian, key could be the uri pattern, value is readable context. basic idea is maping request to some domain object that rate limiter understand
        descriptors:
          - key: auth_type 
          Value: login 
          rate_limit:
            unit: minute 
            requests_per_unit: 5
    
      - store in configuration files and saved on disk or DB
      
    How to response requests that are rate limited(reject/accpet)?
       Exceeding the rate limit 
          - return 429 as HTTP response code
       Rate limiter headers
          X-Ratelimit-Remaining: The remaining number of allowed requests within the window. 
          X-Ratelimit-Limit: It indicates how many calls the client can make per time window.
          X-Ratelimit-Retry-After: The number of seconds to wait until you can make a request again without being throttled.
        
Detailed Design
    
                           / -----------------------> Cache <----------- Workers -------> Configuration Rule
                          /
     Client ----->    Rate limiter middleware ----->  API servers
                         | \
       |                 |  \-------------> Redis
       |                 |
       |<----------------|--------option1----> Request Dropped    
                         |      
                         |---------option2---> Message Queue(optional) ----> API servers
            
        • Rules are stored on the disk. Workers frequently pull rules from the disk and store them in the cache.
        • When a client sends a request to the server, the request is sent to the rate limiter middleware first.
        • Rate limiter middleware loads rules from the cache. It fetches counters and last request timestamp from Redis cache. Based on the response, the rate limiter decides:
          • if the request is not rate limited, it is forwarded to API servers.
          • if the request is rate limited, the rate limiter returns 429 too many requests error to the client. In the meantime, the request is either dropped or forwarded to the queue.



    Rate limiter in a distributed environment
      Race condition
        - issue: two Rate limiter application update on same Redis counter, there will be race condition
          - solution 1) using distributed lock but result in low performance
          - better solution: using the lua script to make INCRE and GET count atomic operation
      Synchronization 
        - in this example, we use the Redis as global storage cache, there is no synchronization issue at all. the issue it is more like distributed session issue where
        request mapping to the different server which server does not know about request

    Performance optimization
      - using the cloud edge technology which help reduce the latency
      - using the cache but need to be careful with eventual consistency issue
  

       
       
       
