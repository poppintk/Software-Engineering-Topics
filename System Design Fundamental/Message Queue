Message Queue:
use case:
   1) Asyc communitation: Producer does need to wait for consumer to finish consuming
   2) Decoupling: decoupling between components so that they can be updated independently
   3) Controll the trafic on peak time, in order to protect downstream services
concepts:
   1) message broker
   2) destination
JMS(Java Message Service) Java base Message queue
- base on JVM message broker standard. ActiveMQ, HornetMQ are JMS implementation

AMQP(advanced Message Queuing Protocol)
- Advanced message queue protocol, JMS compatible
- RabbitMQ is AMQP implementation

RabbitMQ(by default, RabbitMQ supports push model):
   Message:
      message body: not public accessible
      message head: routing-key, priority, delivery-mode(durable)
   Publisher:
      Message producer, send message to exchange of Message queue application
   Exchange:
      use for receiving producer's message and routing those message to correct queue
      4 types: direct(default), fanout, topic, and headers
   
   Queue:
      use for keeping message until messages are sent to consumer. A message can be sent to many queues.
      messages are waiting for consumer to take them away
  
   Binding:
      binding exchange and queue (can be many to many)
      
   Consumer
   
   Virtual host:
      represent whole bunch of exchanges, queue and binding with separant excution environment
   
   Broker:
      many of virtual host together
   
   Connection:
      in Rabbit Queue only on TCP connection is created
   
   Channel:
      use multiplexing bidirectional channel in the TCP connection. AMQP command is sent through channel, 
      Whatever publish, subscribe and receving messages all sent through channel.
      
      
      
      
## Reliable Message Queue
   in order to guarrantee message won't lose, we can use transaction, but throughput will be decreased by a factor of 250
   therefore we use confirm callback instead
   
   NOTE: reliable => need table to store message in the database with status(NOT SEND, SEND, QUEUED, CONSUMED) like a log
   then use callback to keep tracking status. if some message not deliver successful then required scheduler job to re-run.
   
   Producer   -------->   | Broker Exchange ----> Queue | ------>  Consumer

Producer ---> Broker (confirmCallback, triggered as long as broker receives, if cluster only triggers when all broker receive)
Exchange ---> Queue (returnCallback, triggered when message FAIL to deliver to the queue)
Queue ---> Consumer ack
   1) by default auto-ack, as long as consumer consumed then message will be removed from MQ
      issue:
         if more messages delievered and only one message processed, at this moment machine down => message lose
         Solution: consumer manually ack. as long as message not call ack(), the message will be on unacked status. if consumer service down then message will be re-push to MQ
   2) How to ack:
      channel.basicAck(deliveryTag, false); //ack, should be called bussiness logic process successfully
      channel.basicNack(deliveryTag, false, true);// not ack

General issues on MQ?
      1) Message reliable delievery: 
        a) message sent to downstream, but due to network lost
          - retry on "fail to send" messages
          - tracking message via logs and persistence message into disk(can be on database system or RabbitMQ queue)
         b) Message arrive Broker, Broker need to write into disk, but before writting to disk, service down
          - publisher also need callback, after confirm successful then change database message status
         c) Auto-ACK, The consumer received message, but not yet mark ACK, service down
          - Must mannually ACK and after confirming consumed successfully then remove message. if failue then noACK and re-push back to queue
      2) Message duplicated
        a) The consumer consume message successfully and transaction commited(Write operation like POST call to downstream system sent). 
        the machine down right before the ack so that ack not go through. 
        Broker message re-send message from unack status to ready status and send to other consumers
        
        b) message consumed failed but due to retry and double sending message
        soluction: Idempodence
      3) Message Pressure on consumer/subscribe server side
        a) consumer unavaible 
        b) consumer consuming speed less than producer produce
        Soluction:
          - more consumers
          - create message consuming service, firstly batch saving message into database then offline processing
      4) Consumer Server might crash if poll too many message to process(beyond its machine limitation)
         Solution: config the prefetch size base on machine capacity 
          
pattern: eventQueue --exception ocuurs--> retry queue ---max number reached --> Dead letter queue
          
          
 Here are some best practices to ensure that messages are not lost:
- Mark messages as persistent. When publishing a message, set the message's deliveryMode property to 2 to indicate that the message needs to be persistently stored.
- Configure durable queues. When creating a queue, set the durable property to true to indicate that the queue is durable. This ensures that even if the RabbitMQ service crashes or restarts, the queue will not be lost.
- Configure durable exchanges. When creating an exchange, set the durable property to true to indicate that the exchange is durable. This ensures that even if the RabbitMQ service crashes or restarts, the exchange will not be lost.
- Configure backup queues. RabbitMQ allows you to configure backup queues to ensure that messages are not lost. When the main queue fails to handle messages, the messages will be sent to the backup queue to ensure that messages are not lost.
- Enable message acknowledgement. When consuming messages, enable the message acknowledgement mechanism to ensure that messages have been processed correctly. If message processing fails, the message will be resent to other consumers for processing.

 
 How to scale?
   - to scale the throughput for RabbitMQ can use RabbitMq sharding plugin or use nginx to balance load ( scale for throughput)

 Resilience?
   master slave, sync data between master and slave
   
   
   
##################################################################################################
Kafka
   Broker: one kafka server is one broker
   Topic: group of partition (producer can publish to topic and consumer subscribe to it will receive msg)
   Partition: a Topic break down into more of partitions, partition could be distributed in different broker
   Segment: a partition break down into more of segments
   Offset: the sequential id in the partition,  each message in the queue has a sequential id(offset)
   Consumer Group: group of consumers, consumers in a same group can only fetch data from a topic exactly once (there is no overlaping in fetched data).But multiple groups can fetch to same topic at the same time.
   
   Log base Message Broker, detail see the ddia ch11 
   
   Kafka require third party tool for the service discovery, recommmended Zookeeper
   
   Kafka guarrantee the order delivery within a same paritition
   
   Why Kafka allows high throughput?
      - zero copy
      - sequential write
      - pre-fetch
      - post-write (in buffer, then aggregate to disk)
      - segment logs
      - batch process
      - compression 
  
  Where is offset stored in Kafka?
      - eariler version on Zookeeper
      - later version on Kafka cluster(zookeeper topic(_consumer_offset))
      - customized (Redis or DB)
      
  How consumers consume data from the Kafka?
      - Poll (Consumers and Broker system has different throughput, use poll let consumer decide when consumer is ready to take next data)
      
  How Kafka guarantee data won't lose or prevent dulicated consume?
      Producer:
         - Producer sync sending data and ACK = -1(all)
      Broker Cluster:
         - Replicate sync between leader and followers
      Consumer:
         - when Kafka rebalance, data might lose. Solutions?
         - Maintain customized offset(use Lower level API)
         
  Where is Kafka medadata stored?
      - Zookeeper (/controller, /cluster, /consumer /broker)
  
  Why use Kafka?
      - high throughput, sequential order and consumers easy
      
      
  
  
   

