
Use case:
- System requires low real-time and allow eventually consistent
- Read more and write less
- Scale read for syste
- protection for DB in high QPS scenrio like seckill system, because DB normally support lower QPS than cache 

Basic:
   Cache Evition
      decide what to recycle the cache when cache is full
      - LRU
      - FIFO
      - LFU
      
Common cache system issues:
   Cache read issue:
      1) Cache Avalanche
         All cache keys TTL expires at the same time, So all the requests go to DB => DB down
         Solution: Adding randomly TTL to the cache keys

      2) Cache Penetration
        User requests the key does not exist on the DB, so the key must not in the cache as well, So all the requests go to DB => DB down
        Solution: 1)Bloom Filter 2) Allowing null as value to the cache key and also setup TTL.     

      3) Hotspot Invalid
        There are hot data that User keep requesting to the cache(large QPS), right at the point of time TTL get expired, Soall requests go to DB => DB down 
        Solution: Use locks, if distributed then use distributed locks
  

Cache Race Condition under one DB and one Cache:
  Write through: cache update at db write operation 
  Lazy loading: cache update at db read operation
  
  User -----> Cache ----> DB
  
  Write Through/Double write: when data is updated, write to update both DB and cache. (Note: operation order matters)
      example:
         user1 -> writeDB |                             |-> write cache // Cache outdated
         user2 ->           | writeDB  -> write cache   |
   
   Lazy Loading: when data is updated, firstly update DB data then delete cache. (Note: operation order matters)
      example:
         user1 -> writeDB -> delete cache |                          | 
         user2 ->                     |   writeDB    ->         delete cache   |
         user3 ->                           | read cache  not found -> read db | -> update cache
   
   Note: under high concurrency senerio both cases will cause data inconsistency. adding TTL to make it eventually consistent.
   
   
   - what if system requires strong consistency? (depends on senerio)
      - Distributed locking service will reduce efficiency(network call)
      - use Canal for MySQL
      - Write more read less senerio, use read from DB directly
      
      conclusion: 
         if system write more and read less requires strong consistency => no cache 
         if system read more and write less requires storng consistency => use distributed lock or Canal for MYSQL
      
   Best practice: lazy loading: database.set(key,user); cache.delete(key)


Redis Cluster:
   1) Sharding
      a) client side sharding
         pros: easy
         cons: if one Redis down client side don't know
      b) Proxy sharding
         
      c) Redis-cluster(offical support)
          with consistent hashing
  2) Replications
      Redis-cluster(offical support)

  
  
Distributed Lock (Redison - Redis based distributed lock framework)
   - Reentrance Lock
   - ReadWrite Lock
   - Semaphore
   - CountDownLatch
   
   
Spring cache
   cons:
      cache read:
         - Cache Penetration: cache-null-value = true
         - Hotspot Invalid: local locking, by default no locking, require set sync = true
         - Cache Avalanche: use only TTL 
      cache write(cache update):
         - use only TTL for eventually consistent
   

Distributed Session issue:
   1) cannot share on different domain
   2) Load balancing to arbitrary machine and that machine not guarrantee have the previous stored session
   



Distributed environment
   Sychronization issue
      When multiple instances are used, then it is important to study whether there is issue that mapping to a different instance as before. 
      Distributed session issue, it's example of this issue
      
      Solution:
         1) ensure request is alwasy mapping to same instance as before
            - sticky session (idea ensure)
            - consistent hashing
         2) create a global unique instance
            - we could use some consensus algorithm to ensure instance act like one machine
         
   Race Condition/concurrent issue(occurs on update cache, as example above)
      case that under high concurrency (multiple request in any order), the ouput is randomly generated depends the which
      operation go in first (like Redis for example, GET and INCRE are separate operations if not using LUA script then item could be not increment appropriate)
      
      Solutions:
         - Distributed lock -> low performance
         - Distributed Transaction
      
   Distributed transaction issue
      Under distributed environment, one operation contains multiple DB objects
       Solutions:
         - Distributed Transaction
