Distributed Cache:

Use case:
- System requires low real-time and allow eventually consistent
- Read more and write less

Common cache system issues:
   Cache read issue:
      1)缓存雪崩 Cache Avalanche
         All cache keys TTL expires at the same time, So all the requests go to DB => DB down
         Solution: Adding randomly TTL to the cache keys

      2)缓存穿透 Cache Penetration
        User requests the key does not exist on the DB, so the key must not in the cache as well, So all the requests go to DB => DB down
        Solution: 1)Bloom Filter 2) Allowing null as value to the cache key and also setup TTL.     

      3)缓存击穿 Hotspot Invalid
        There are hot data that User keep requesting to the cache(large QPS), right at the point of time TTL get expired, Soall requests go to DB => DB down 
        Solution: Use locks, if distributed then use distributed locks
  
  
Cache update issues(write):
   Double write: when data is updated, write to update both DB and cache. 
   Lazy Loading: when data is updated, firstly update DB data then delete cache.
   under high concurrency senerio both cases will cause data inconsistency. adding TTL to make it eventually consistent.
   
   - what if system requires strong consistency? (depends on senerio)
      - Distributed locking service will reduce efficiency(network call)
      - use Canal for MySQL
      - Write more read less senerio, use read from DB directly
      
      conclusion: 
         if system write more and read less requires strong consistency => no cache 
         if system read more and write less requires storng consistency => use distributed lock or Canal for MYSQL
      
   Best practice: lazy loading: database.set(key,user); cache.delete(key)
   
   
   
  
Distributed Lock (Redison - Redis based distributed lock framework)
   - Reentrance Lock
   - ReadWrite Lock
   - Semaphore
   - CountDownLatch
   
   
Spring cache
   cons:
      cache read:
         - Cache Penetration: cache-null-value = true
         - Hotspot Invalid: local locking
         - Cache Avalanche: use only TTL 
      cache write(cache update):
         - 
   
   
Distributed Session issue:
   1) cannot share on different domain
   2) Load balancing to arbitrary machine and that machine not guarrantee have the previous stored session
   
   
Oauth2 flow for web server:
   1) client browser click on the Oauth2 login icon, Request to third party for the login with redirect URL(for web server)
   2) Users input their third party login
   3) After authenticate third party authorization server redirect back to the URL(web server) with access token(code)
   4) The web server now can call POST request ask for public protected user information. Note the POST body requires access token(code) just return
   CLIENT_ID, CLIENT_KEY those can be access from the third party
   5) public protected information is returned
   
   
   
   
 Distributed session issues:
   1) different domains, session cannot be shared among different domains
      but session can be share among its sub-domain(zi yun ming)
   
   2) in distributed system, load balancer might routing to different machine, and current machine might not have session.
      solution: 
         1) session replicate syncronously between web servers.
            pros:
               web-server(tomcat) supports it, need to config
            cons:
               - session replecate synconously need network transmission as result lower the cluster performance
               - session cannot horizontally scale if session size too big
               - in distributed system, this approach not work well
          2) save session on client side
             pros:
               web server now does not need to sotre session inforamtion. uses save their session information on cookie.
             
             cons:
               - each every session need to be stored data insize cookie, cookie size limitation is 4KB, cannot store big inforamtion
               - cookie is unsecured and likely be hacked
           
           3) hash consistent so that every request will hash to same web server
               pros: 
                  - only required nginx configration, no code required
                  - all web server horizontally scale
               cons:
                  - when adding/remove new webservers rehashing required, and some users might hash to wrong session
                  
            4) Storage: Database/Redis
               Pros:
                  - safe
                  - allow horizontally scale (sharding by key)
                  - web server restart wont lost data
                  
               cons:
                  - required additional network access.
                  - Redis get data still slower than get data from memory directly
      
               
          
