Async/Sync VS Blocking/Non-blocking 
    Main difference: Callee & Caller 

    1) Async/Sync: Whether or not Caller will wait for Callee to finish callee's execution. 
        Sync means Caller will wait for Callee to finish 
        Async means Caller will NOT wait for Callee to finish and Caller continue to execute

    2) Blocking/Non-blocking: Whether or not Callee will return immediately to Caller. 
        Blocking means Callee will NOT return immediately to Caller, Callee will execute its logic then return result to Caller.
        Non-Blocking means Callee will return immediately to Caller, Callee will return immediately back to Caller (Callee might be half way on its execution OR not        yet started OR finish execuate its logic)


SpringBoot Tomcat server config
    maxConnection: the number of client requests will be taken by the Tomcat server
    maxThread: the number of threads that will be asigned to handle the client request
    acceptCount: when maxConnection is full then client requests will be queued in acceptCount, if acceptCount is full then exceeding requests will be rejected

------------------------------------------------------



Reactive Programming
    
    - Observable + Subscriber + Subscription (Webflux moving away from Thread per Request Model)
    
    - Asynchronous and Non Blocking 
        - (Note: if your application has blocking logic, it will wait for blocking logic to be executed, so don't mix iwth blocking logic with Reactor together)
        - Reactive runtimes typically use a limited number of threads that match the number of CPU cores
    
    - Functional Programming
    
    - CompleteFuture vs Reactor
        CompleteFuture
            It will create a sub thread to execute the subtask immediately
        Reactor
            it will create a subthread to execute the subtask only when it is subscribed(Lazy)

    - BackPressure:
        Senerio:
        The speed that data pushed by publisher is faster than subscriber can pull
        
        If we imagine we were being streamed tweets from twitter, it would then be up to the upstream(Services send tweets) to decide what to do. If tweets were coming in but there are no requests from the downstream(Services consume the tweets), then the upstream could drop items, store them in a buffer, or some other strategy.

        Essentially, this is reactive pull backpressure. We are requesting the upstream(producer) to only push a certain amount of elements, and only when we are ready.



Spring MVC
    Request/Response Flow:            |--------------------------------------|
        Request(->)/Response(<-) <->  |Servelet Container(Tomacat) ThreadPool|  <-> thread1 <-> Filter <-> Dispacher Servlet <-> RequestMapping and HandlerMapping <-> Controller <-> Service <-> Remote Database/Remote API       
    
    Limit on the number of Concurrent users
        Thread Per Request Model: each request come in with one thread to handle
        configure through server.tomcat.max-threads
        by default: 200 concurrent connections
        
    Synchronous and Blocking:
        - each thread take some space in memory and horizontal scaling requires more machines(additional cost)
        - if there is I/O operation like reading something from DB then need to wait, and current thread must wait for reading db operation done
        cannot be used by other things (inefficent use of resources)
    
        Better use with:
            - Callback and Future
                - Callback pass the method as parameter and bad for code maintainability(Complex)
                - Future hard to compose multiple asynchronous operations
            - Completable Future
                - Easy to compose multiple Asynchronous operations
                - Not a great fit asynchronous call with multiple items
            
   No Back Pressure support, say we have one API will return all items. Now there are huge data set in database.
   if read them all application will be crashed with OUT OF MEMORY error and client might be overwhelmed with huge data
        
     
Spring WebFlux(Moving away from Thread Per Request Model)
    How it works?
        Request/Response <-> Server <-> RouterFunction <-> HandlerFunction
            RouterFunction is similar to @RequestMapping
            HandlerFunction is similar to body of method which be annotated with "@RequestMapping", it has parameters of ServerRequest and ServerResponse
        
    Specification
        1) Publisher
            - represent data source
            public interface Publiser<T> {
                public void subscribe(Subscriber<? super T> s);
            }
        2) Subscriber
        
            public interface Subscriber<T> {
                public void onNext(T t);
                public void onError(Throwable t);
                public void onComplete();
                public void onSubscribe(Subscription s);
            }
            
        3) Subscription
            public interface Subscription {
                public void request(long n);
                public void cancel();
            }
            
            
        Publisher/Subscriber Event Flow
            1) Subscriber subscribe to Publisher
            
            // step 2-3 is to make contract between Subsciber and Publisher how much data to send
            2) Publisher sent Subscription object to Subscriber
            3) Subscriber request(n) to Publisher, by default request all data in data stream (Back pressure)
            
            4) Publisher sent onNext(data) to Subscriber for n event data
            5) Publisher call onComplete to Subscriber after all data sent
        
        4) Processor
            public interface Processor<T, R> extends Subscriber<T>, Publiser<R> {}


